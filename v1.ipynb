{"cells":[{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torchvision.models import resnet50\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on NVIDIA GeForce RTX 3080\n"]}],"source":["# Check if CUDA is available\n","if torch.cuda.is_available():\n","    # Set the default tensor type to CUDA tensors\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","\n","    # Define the device as the first visible cuda device if available\n","    device = torch.device(\"cuda:0\")\n","    print(f\"Running on {torch.cuda.get_device_name(device)}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on CPU\")\n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# Data loaders for your deep fake and genuine images dataset\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    # Normalization with ImageNet mean and std\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["# Updated paths for the datasets\n","train_dataset_path = \"C:/Users/Henry/Downloads/archive/Dataset/train\"\n","val_dataset_path = \"C:/Users/Henry/Downloads/archive/Dataset/validation\"\n","# test_dataset_path = \"C:/Users/Henry/Downloads/archive/Dataset/test\"  # Uncomment this if you have a test dataset\n","\n","# Ensure train_transform is defined correctly\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    # Add other transformations as needed\n","])\n","\n","# Load datasets\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=train_transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, generator=torch.Generator(device='cuda'))\n","\n","val_dataset = datasets.ImageFolder(val_dataset_path, transform=train_transform)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n","\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Henry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Users\\Henry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["# Initialize ResNet model and modify the last layer\n","model = resnet50(pretrained=True)\n","in_features = model.fc.in_features\n","model.fc = nn.Linear(in_features, 2)  # Assuming 2 classes: genuine and deep fake\n","model = model.to(device)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Step [100/4376], Loss: 0.3308\n","Epoch [1/10], Step [200/4376], Loss: 0.1084\n","Epoch [1/10], Step [300/4376], Loss: 0.1632\n","Epoch [1/10], Step [400/4376], Loss: 0.1661\n","Epoch [1/10], Step [500/4376], Loss: 0.0956\n","Epoch [1/10], Step [600/4376], Loss: 0.0843\n","Epoch [1/10], Step [700/4376], Loss: 0.1635\n","Epoch [1/10], Step [800/4376], Loss: 0.1375\n","Epoch [1/10], Step [900/4376], Loss: 0.1159\n","Epoch [1/10], Step [1000/4376], Loss: 0.1403\n","Epoch [1/10], Step [1100/4376], Loss: 0.1325\n","Epoch [1/10], Step [1200/4376], Loss: 0.2016\n","Epoch [1/10], Step [1300/4376], Loss: 0.1030\n","Epoch [1/10], Step [1400/4376], Loss: 0.0526\n","Epoch [1/10], Step [1500/4376], Loss: 0.0818\n","Epoch [1/10], Step [1600/4376], Loss: 0.1927\n","Epoch [1/10], Step [1700/4376], Loss: 0.1011\n","Epoch [1/10], Step [1800/4376], Loss: 0.0543\n","Epoch [1/10], Step [1900/4376], Loss: 0.0775\n","Epoch [1/10], Step [2000/4376], Loss: 0.0760\n","Epoch [1/10], Step [2100/4376], Loss: 0.1158\n","Epoch [1/10], Step [2200/4376], Loss: 0.1335\n","Epoch [1/10], Step [2300/4376], Loss: 0.1073\n","Epoch [1/10], Step [2400/4376], Loss: 0.1268\n","Epoch [1/10], Step [2500/4376], Loss: 0.0351\n","Epoch [1/10], Step [2600/4376], Loss: 0.0239\n","Epoch [1/10], Step [2700/4376], Loss: 0.1528\n","Epoch [1/10], Step [2800/4376], Loss: 0.3028\n","Epoch [1/10], Step [2900/4376], Loss: 0.0773\n","Epoch [1/10], Step [3000/4376], Loss: 0.2644\n","Epoch [1/10], Step [3100/4376], Loss: 0.0688\n","Epoch [1/10], Step [3200/4376], Loss: 0.0079\n","Epoch [1/10], Step [3300/4376], Loss: 0.0139\n","Epoch [1/10], Step [3400/4376], Loss: 0.1442\n","Epoch [1/10], Step [3500/4376], Loss: 0.1291\n","Epoch [1/10], Step [3600/4376], Loss: 0.0783\n","Epoch [1/10], Step [3700/4376], Loss: 0.0645\n","Epoch [1/10], Step [3800/4376], Loss: 0.2169\n","Epoch [1/10], Step [3900/4376], Loss: 0.0490\n","Epoch [1/10], Step [4000/4376], Loss: 0.0300\n","Epoch [1/10], Step [4100/4376], Loss: 0.0358\n","Epoch [1/10], Step [4200/4376], Loss: 0.0729\n","Epoch [1/10], Step [4300/4376], Loss: 0.0522\n"]}],"source":["for epoch in range(1):  # 10 epochs, adjust as needed\n","    model.train()\n","    for i, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"Epoch [{epoch + 1}/10], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 94.68651719590139%\n"]}],"source":["# Validation Loop\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in val_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","\n","    print(f\"Validation Accuracy: {100 * correct / total}%\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1909705,"sourceId":3134515,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
