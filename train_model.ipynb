{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Henry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from efficientnet_pytorch import EfficientNet"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on NVIDIA GeForce RTX 3080\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Henry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n","  _C._set_default_tensor_type(t)\n"]}],"source":["# Check if CUDA is available\n","if torch.cuda.is_available():\n","    # Set the default tensor type to CUDA tensors\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","\n","    # Define the device as the first visible cuda device if available\n","    device = torch.device(\"cuda:0\")\n","    print(f\"Running on {torch.cuda.get_device_name(device)}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on CPU\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Data augmentation for the training set\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomHorizontalFlip(),  # Randomly flip the images on the horizontal axis\n","    transforms.RandomRotation(10),      # Random rotation of the images by 10 degrees\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly changing brightness, contrast, and saturation\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_dataset_path = \"C:/Users/Henry/Downloads/archive/Dataset/train\"\n","val_dataset_path = \"C:/Users/Henry/Downloads/archive/Dataset/validation\"\n","test_dataset_path = \"C:/Users/Henry/Downloads/archive/Dataset/test\" \n","\n","# Data loaders\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# Load datasets\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=train_transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, generator=torch.Generator(device='cuda'))\n","\n","val_dataset = datasets.ImageFolder(val_dataset_path, transform=train_transform)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8)\n","\n","test_dataset = datasets.ImageFolder(test_dataset_path, transform=train_transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b3\n"]}],"source":["# Model\n","model_name = 'efficientnet-b3'\n","model = EfficientNet.from_pretrained(model_name)\n","\n","# Adding Dropout layer\n","dropout_rate = 0.5  # You can adjust this value\n","model._dropout = nn.Dropout(p=dropout_rate)\n","\n","# Modify the Final Layer for binary classification\n","num_classes = 2\n","in_features = model._fc.in_features\n","model._fc = nn.Linear(in_features, num_classes)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Loss and Optimizer with L2 Regularization (Weight Decay)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/1], Step [100/4376], Loss: 0.2489\n","Epoch [1/1], Step [200/4376], Loss: 0.0521\n","Epoch [1/1], Step [300/4376], Loss: 0.1369\n","Epoch [1/1], Step [400/4376], Loss: 0.0998\n","Epoch [1/1], Step [500/4376], Loss: 0.1661\n","Epoch [1/1], Step [600/4376], Loss: 0.0123\n","Epoch [1/1], Step [700/4376], Loss: 0.0084\n","Epoch [1/1], Step [800/4376], Loss: 0.0767\n","Epoch [1/1], Step [900/4376], Loss: 0.0717\n","Epoch [1/1], Step [1000/4376], Loss: 0.1912\n","Epoch [1/1], Step [1100/4376], Loss: 0.1155\n","Epoch [1/1], Step [1200/4376], Loss: 0.0249\n","Epoch [1/1], Step [1300/4376], Loss: 0.0849\n","Epoch [1/1], Step [1400/4376], Loss: 0.0734\n","Epoch [1/1], Step [1500/4376], Loss: 0.0665\n","Epoch [1/1], Step [1600/4376], Loss: 0.0830\n","Epoch [1/1], Step [1700/4376], Loss: 0.0277\n","Epoch [1/1], Step [1800/4376], Loss: 0.0093\n","Epoch [1/1], Step [1900/4376], Loss: 0.0339\n","Epoch [1/1], Step [2000/4376], Loss: 0.0654\n","Epoch [1/1], Step [2100/4376], Loss: 0.0184\n","Epoch [1/1], Step [2200/4376], Loss: 0.0880\n","Epoch [1/1], Step [2300/4376], Loss: 0.0326\n","Epoch [1/1], Step [2400/4376], Loss: 0.0765\n","Epoch [1/1], Step [2500/4376], Loss: 0.0113\n","Epoch [1/1], Step [2600/4376], Loss: 0.0463\n","Epoch [1/1], Step [2700/4376], Loss: 0.1398\n","Epoch [1/1], Step [2800/4376], Loss: 0.1728\n","Epoch [1/1], Step [2900/4376], Loss: 0.1220\n","Epoch [1/1], Step [3000/4376], Loss: 0.1074\n","Epoch [1/1], Step [3100/4376], Loss: 0.0916\n","Epoch [1/1], Step [3200/4376], Loss: 0.0745\n","Epoch [1/1], Step [3300/4376], Loss: 0.0168\n","Epoch [1/1], Step [3400/4376], Loss: 0.0562\n","Epoch [1/1], Step [3500/4376], Loss: 0.0752\n","Epoch [1/1], Step [3600/4376], Loss: 0.2943\n","Epoch [1/1], Step [3700/4376], Loss: 0.0059\n","Epoch [1/1], Step [3800/4376], Loss: 0.0691\n","Epoch [1/1], Step [3900/4376], Loss: 0.0171\n","Epoch [1/1], Step [4000/4376], Loss: 0.0132\n","Epoch [1/1], Step [4100/4376], Loss: 0.0088\n","Epoch [1/1], Step [4200/4376], Loss: 0.1448\n","Epoch [1/1], Step [4300/4376], Loss: 0.0349\n"]}],"source":["# Training Loop\n","num_epochs = 1\n","for epoch in range(num_epochs):  \n","    model.train()\n","    for i, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n","\n","    # Save the model at the end of each epoch\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss.item(),\n","    }, f\"model_epoch_{epoch+1}.pth\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 95.98508674038754%\n"]}],"source":["best_val_accuracy = 0\n","# Validation Loop\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in val_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    current_val_accuracy = 100 * correct / total\n","    print(f\"Validation Accuracy: {100 * correct / total}%\")\n","    # Save the model if it has the best validation accuracy so far\n","    if current_val_accuracy > best_val_accuracy:\n","        best_val_accuracy = current_val_accuracy"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 84.16322787712059%\n"]}],"source":["# Test Loop\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print(f\"Test Accuracy: {100 * correct / total}%\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1909705,"sourceId":3134515,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
